"""
Hyperparameter tuning for vulnerability detection models.
Optimizes models to achieve F1-score >= 0.85 and FPR < 0.10.
"""

import sys
import os
import argparse
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score, confusion_matrix
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
import numpy as np
import pandas as pd
import joblib
import logging
from datetime import datetime

from data_loader import ContractDataLoader
from feature_extractor import FeatureExtractor

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def tune_xgboost(X_train, y_train, cv_folds=5):
    """Tune XGBoost hyperparameters."""
    logger.info("Tuning XGBoost...")
    
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [3, 5, 7],
        'learning_rate': [0.01, 0.1, 0.2],
        'subsample': [0.8, 0.9, 1.0],
        'colsample_bytree': [0.8, 0.9, 1.0],
        'min_child_weight': [1, 3, 5]
    }
    
    # Optimize for F1-score
    f1_scorer = make_scorer(f1_score)
    
    xgb = XGBClassifier(
        random_state=42,
        class_weight='balanced',
        eval_metric='logloss',
        n_jobs=-1
    )
    
    grid_search = GridSearchCV(
        xgb,
        param_grid,
        cv=cv_folds,
        scoring=f1_scorer,
        n_jobs=-1,
        verbose=2
    )
    
    logger.info("Starting grid search (this may take a while)...")
    grid_search.fit(X_train, y_train)
    
    logger.info(f"Best parameters: {grid_search.best_params_}")
    logger.info(f"Best CV F1-score: {grid_search.best_score_:.4f}")
    
    return grid_search.best_estimator_, grid_search.best_params_


def tune_random_forest(X_train, y_train, cv_folds=5):
    """Tune Random Forest hyperparameters."""
    logger.info("Tuning Random Forest...")
    
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [10, 20, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4],
        'max_features': ['sqrt', 'log2', None]
    }
    
    f1_scorer = make_scorer(f1_score)
    
    rf = RandomForestClassifier(
        random_state=42,
        class_weight='balanced',
        n_jobs=-1
    )
    
    grid_search = GridSearchCV(
        rf,
        param_grid,
        cv=cv_folds,
        scoring=f1_scorer,
        n_jobs=-1,
        verbose=2
    )
    
    logger.info("Starting grid search (this may take a while)...")
    grid_search.fit(X_train, y_train)
    
    logger.info(f"Best parameters: {grid_search.best_params_}")
    logger.info(f"Best CV F1-score: {grid_search.best_score_:.4f}")
    
    return grid_search.best_estimator_, grid_search.best_params_


def tune_logistic_regression(X_train, y_train, cv_folds=5):
    """Tune Logistic Regression hyperparameters."""
    logger.info("Tuning Logistic Regression...")
    
    param_grid = {
        'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],
        'penalty': ['l1', 'l2'],
        'solver': ['liblinear', 'saga']
    }
    
    f1_scorer = make_scorer(f1_score)
    
    lr = LogisticRegression(
        random_state=42,
        class_weight='balanced',
        max_iter=1000
    )
    
    grid_search = GridSearchCV(
        lr,
        param_grid,
        cv=cv_folds,
        scoring=f1_scorer,
        n_jobs=-1,
        verbose=2
    )
    
    logger.info("Starting grid search...")
    grid_search.fit(X_train, y_train)
    
    logger.info(f"Best parameters: {grid_search.best_params_}")
    logger.info(f"Best CV F1-score: {grid_search.best_score_:.4f}")
    
    return grid_search.best_estimator_, grid_search.best_params_


def evaluate_model(model, X_test, y_test, model_name):
    """Evaluate model and return metrics."""
    from sklearn.metrics import classification_report, confusion_matrix
    
    y_pred = model.predict(X_test)
    
    # Calculate metrics
    f1 = f1_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    
    # Calculate FPR
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0
    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0
    
    logger.info(f"\n=== {model_name} Test Results ===")
    logger.info(f"F1-score: {f1:.4f}")
    logger.info(f"Precision: {precision:.4f}")
    logger.info(f"Recall: {recall:.4f}")
    logger.info(f"Accuracy: {accuracy:.4f}")
    logger.info(f"False Positive Rate: {fpr:.4f}")
    
    # Check if meets targets
    if f1 >= 0.85 and fpr < 0.10:
        logger.info("✓ Meets target: F1 >= 0.85 and FPR < 0.10")
    else:
        logger.info("⚠ Does not fully meet target")
        if f1 < 0.85:
            logger.info(f"  - F1-score {f1:.4f} < 0.85")
        if fpr >= 0.10:
            logger.info(f"  - FPR {fpr:.4f} >= 0.10")
    
    return {
        'f1_score': f1,
        'precision': precision,
        'recall': recall,
        'accuracy': accuracy,
        'false_positive_rate': fpr
    }


def main():
    """Main execution function."""
    parser = argparse.ArgumentParser(description='Hyperparameter tuning for vulnerability detection')
    parser.add_argument('--data-dir', type=str, default='data', help='Data directory')
    parser.add_argument('--smartbugs-dir', type=str, default=None, help='SmartBugs results directory')
    parser.add_argument('--max-contracts', type=int, default=2000, help='Max contracts to use')
    parser.add_argument('--cv-folds', type=int, default=5, help='Cross-validation folds')
    parser.add_argument('--models-dir', type=str, default='models', help='Models directory')
    parser.add_argument('--skip-tuning', action='store_true', help='Skip tuning, only evaluate')
    
    args = parser.parse_args()
    
    # Load data
    logger.info("Loading data...")
    data_loader = ContractDataLoader(
        data_dir=args.data_dir,
        smartbugs_dir=args.smartbugs_dir
    )
    
    # Try to load with real labels first
    try:
        contracts, labels = data_loader.load_dataset_with_real_labels(
            max_contracts=args.max_contracts,
            smartbugs_dir=args.smartbugs_dir
        )
        logger.info("Loaded dataset with real labels")
    except:
        contracts, labels = data_loader.load_dataset(max_contracts=args.max_contracts)
        logger.info("Loaded dataset (using synthetic labels)")
    
    X_train_raw, X_test_raw, y_train, y_test = data_loader.get_data_split(
        contracts, labels, test_size=0.2, random_state=42
    )
    
    # Extract features
    logger.info("Extracting features...")
    feature_extractor = FeatureExtractor()
    X_train = feature_extractor.extract_features(X_train_raw)
    X_test = feature_extractor.extract_features(X_test_raw)
    
    logger.info(f"Feature matrix shape: {X_train.shape}")
    logger.info(f"Number of features: {X_train.shape[1]}")
    
    os.makedirs(args.models_dir, exist_ok=True)
    
    results = {}
    best_params = {}
    
    # Tune and evaluate models
    if not args.skip_tuning:
        # XGBoost
        logger.info("\n" + "="*60)
        logger.info("XGBOOST")
        logger.info("="*60)
        xgb_model, xgb_params = tune_xgboost(X_train, y_train, args.cv_folds)
        best_params['xgboost'] = xgb_params
        xgb_metrics = evaluate_model(xgb_model, X_test, y_test, "XGBoost")
        results['xgboost'] = xgb_metrics
        joblib.dump(xgb_model, os.path.join(args.models_dir, 'xgboost_tuned.pkl'))
        
        # Random Forest
        logger.info("\n" + "="*60)
        logger.info("RANDOM FOREST")
        logger.info("="*60)
        rf_model, rf_params = tune_random_forest(X_train, y_train, args.cv_folds)
        best_params['random_forest'] = rf_params
        rf_metrics = evaluate_model(rf_model, X_test, y_test, "Random Forest")
        results['random_forest'] = rf_metrics
        joblib.dump(rf_model, os.path.join(args.models_dir, 'random_forest_tuned.pkl'))
        
        # Logistic Regression
        logger.info("\n" + "="*60)
        logger.info("LOGISTIC REGRESSION")
        logger.info("="*60)
        lr_model, lr_params = tune_logistic_regression(X_train, y_train, args.cv_folds)
        best_params['logistic_regression'] = lr_params
        lr_metrics = evaluate_model(lr_model, X_test, y_test, "Logistic Regression")
        results['logistic_regression'] = lr_metrics
        joblib.dump(lr_model, os.path.join(args.models_dir, 'logistic_regression_tuned.pkl'))
    else:
        logger.info("Skipping tuning, loading existing models...")
        # Load and evaluate existing models
        xgb_model = joblib.load(os.path.join(args.models_dir, 'xgboost_tuned.pkl'))
        results['xgboost'] = evaluate_model(xgb_model, X_test, y_test, "XGBoost")
        
        rf_model = joblib.load(os.path.join(args.models_dir, 'random_forest_tuned.pkl'))
        results['random_forest'] = evaluate_model(rf_model, X_test, y_test, "Random Forest")
        
        lr_model = joblib.load(os.path.join(args.models_dir, 'logistic_regression_tuned.pkl'))
        results['logistic_regression'] = evaluate_model(lr_model, X_test, y_test, "Logistic Regression")
    
    # Save results
    results_dir = 'results'
    os.makedirs(results_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    results_df = pd.DataFrame(results).T
    results_path = os.path.join(results_dir, f'tuning_results_{timestamp}.csv')
    results_df.to_csv(results_path)
    logger.info(f"\nSaved results to {results_path}")
    
    if best_params:
        best_params_df = pd.DataFrame(best_params).T
        params_path = os.path.join(results_dir, f'best_params_tuning_{timestamp}.csv')
        best_params_df.to_csv(params_path)
        logger.info(f"Saved best parameters to {params_path}")
    
    # Summary
    logger.info("\n" + "="*60)
    logger.info("SUMMARY")
    logger.info("="*60)
    logger.info(f"\nBest F1-score: {results_df['f1_score'].max():.4f}")
    logger.info(f"Best model: {results_df['f1_score'].idxmax()}")
    logger.info(f"\nLowest FPR: {results_df['false_positive_rate'].min():.4f}")
    logger.info(f"Model with lowest FPR: {results_df['false_positive_rate'].idxmin()}")
    
    # Check target achievement
    logger.info("\n" + "="*60)
    logger.info("TARGET ACHIEVEMENT")
    logger.info("="*60)
    for model_name, metrics in results.items():
        f1 = metrics['f1_score']
        fpr = metrics['false_positive_rate']
        meets_target = f1 >= 0.85 and fpr < 0.10
        status = "✓" if meets_target else "⚠"
        logger.info(f"{status} {model_name}: F1={f1:.4f}, FPR={fpr:.4f}")


if __name__ == "__main__":
    main()

