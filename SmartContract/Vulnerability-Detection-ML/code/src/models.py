"""
Model training and evaluation module.
Implements Logistic Regression, Random Forest, and XGBoost classifiers.
"""

import numpy as np
import pandas as pd
from typing import Dict
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold
from sklearn.metrics import (
    f1_score, precision_score, recall_score, 
    confusion_matrix, classification_report, roc_auc_score
)
import logging
import joblib
import os

try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except (ImportError, Exception) as e:
    XGBOOST_AVAILABLE = False
    xgb = None
    logging.warning(f"XGBoost not available: {e}. XGBoost training will be skipped.")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ModelTrainer:
    """Trains and evaluates machine learning models for vulnerability detection."""
    
    def __init__(self, models_dir: str = "models"):
        """
        Initialize the model trainer.
        
        Args:
            models_dir: Directory to save trained models
        """
        self.models_dir = models_dir
        os.makedirs(models_dir, exist_ok=True)
        
        self.models = {}
        self.best_params = {}
        self.cv_scores = {}
        
    def train_logistic_regression(self, X_train: np.ndarray, y_train: np.ndarray,
                                  cv_folds: int = 5) -> LogisticRegression:
        """
        Train Logistic Regression model with hyperparameter tuning.
        
        Args:
            X_train: Training features
            y_train: Training labels
            cv_folds: Number of cross-validation folds
            
        Returns:
            Trained LogisticRegression model
        """
        logger.info("Training Logistic Regression model...")
        
        # Hyperparameter grid
        # Note: liblinear supports both l1 and l2, saga also supports both but is slower
        param_grid = [
            {
                'C': [0.1, 1.0, 10.0, 100.0],
                'penalty': ['l1'],
                'solver': ['liblinear'],
                'max_iter': [1000, 2000]
            },
            {
                'C': [0.1, 1.0, 10.0, 100.0],
                'penalty': ['l2'],
                'solver': ['liblinear', 'lbfgs'],
                'max_iter': [1000, 2000]
            }
        ]
        
        base_model = LogisticRegression(random_state=42, class_weight='balanced')
        
        # Grid search with cross-validation
        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
        grid_search = GridSearchCV(
            base_model, param_grid, cv=cv, 
            scoring='f1', n_jobs=-1, verbose=1
        )
        
        grid_search.fit(X_train, y_train)
        
        best_model = grid_search.best_estimator_
        self.best_params['logistic_regression'] = grid_search.best_params_
        self.cv_scores['logistic_regression'] = grid_search.best_score_
        
        logger.info(f"Best parameters: {grid_search.best_params_}")
        logger.info(f"Best CV F1-score: {grid_search.best_score_:.4f}")
        
        self.models['logistic_regression'] = best_model
        return best_model
    
    def train_random_forest(self, X_train: np.ndarray, y_train: np.ndarray,
                            cv_folds: int = 5) -> RandomForestClassifier:
        """
        Train Random Forest model with hyperparameter tuning.
        
        Args:
            X_train: Training features
            y_train: Training labels
            cv_folds: Number of cross-validation folds
            
        Returns:
            Trained RandomForestClassifier model
        """
        logger.info("Training Random Forest model...")
        
        # Hyperparameter grid
        param_grid = {
            'n_estimators': [100, 200, 300],
            'max_depth': [10, 20, 30, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'class_weight': ['balanced', None]
        }
        
        base_model = RandomForestClassifier(random_state=42, n_jobs=-1)
        
        # Grid search with cross-validation
        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
        grid_search = GridSearchCV(
            base_model, param_grid, cv=cv,
            scoring='f1', n_jobs=-1, verbose=1
        )
        
        grid_search.fit(X_train, y_train)
        
        best_model = grid_search.best_estimator_
        self.best_params['random_forest'] = grid_search.best_params_
        self.cv_scores['random_forest'] = grid_search.best_score_
        
        logger.info(f"Best parameters: {grid_search.best_params_}")
        logger.info(f"Best CV F1-score: {grid_search.best_score_:.4f}")
        
        self.models['random_forest'] = best_model
        return best_model
    
    def train_xgboost(self, X_train: np.ndarray, y_train: np.ndarray,
                      cv_folds: int = 5):
        """
        Train XGBoost model with hyperparameter tuning.
        
        Args:
            X_train: Training features
            y_train: Training labels
            cv_folds: Number of cross-validation folds
            
        Returns:
            Trained XGBClassifier model
        """
        if not XGBOOST_AVAILABLE or xgb is None:
            raise ImportError("XGBoost is not available. Please install it with: pip install xgboost")
        
        logger.info("Training XGBoost model...")
        
        # Hyperparameter grid
        param_grid = {
            'n_estimators': [100, 200, 300],
            'max_depth': [3, 5, 7, 10],
            'learning_rate': [0.01, 0.1, 0.2],
            'subsample': [0.8, 1.0],
            'colsample_bytree': [0.8, 1.0],
            'scale_pos_weight': [1, 2, 4]  # For imbalanced data
        }
        
        base_model = xgb.XGBClassifier(
            random_state=42, 
            eval_metric='logloss'
        )
        
        # Grid search with cross-validation
        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
        grid_search = GridSearchCV(
            base_model, param_grid, cv=cv,
            scoring='f1', n_jobs=-1, verbose=1
        )
        
        grid_search.fit(X_train, y_train)
        
        best_model = grid_search.best_estimator_
        self.best_params['xgboost'] = grid_search.best_params_
        self.cv_scores['xgboost'] = grid_search.best_score_
        
        logger.info(f"Best parameters: {grid_search.best_params_}")
        logger.info(f"Best CV F1-score: {grid_search.best_score_:.4f}")
        
        self.models['xgboost'] = best_model
        return best_model
    
    def evaluate_model(self, model, X_test: np.ndarray, y_test: np.ndarray,
                      model_name: str = "model") -> Dict[str, float]:
        """
        Evaluate a trained model on test data.
        
        Args:
            model: Trained model
            X_test: Test features
            y_test: Test labels
            model_name: Name of the model for logging
            
        Returns:
            Dictionary of evaluation metrics
        """
        logger.info(f"Evaluating {model_name}...")
        
        # Predictions
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None
        
        # Calculate metrics
        f1 = f1_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        
        # Confusion matrix
        cm = confusion_matrix(y_test, y_pred)
        tn, fp, fn, tp = cm.ravel()
        
        # False positive rate
        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0
        
        # Accuracy
        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0
        
        # ROC-AUC if probabilities available
        roc_auc = None
        if y_pred_proba is not None:
            try:
                roc_auc = roc_auc_score(y_test, y_pred_proba)
            except:
                pass
        
        metrics = {
            'f1_score': f1,
            'precision': precision,
            'recall': recall,
            'accuracy': accuracy,
            'false_positive_rate': fpr,
            'true_positives': int(tp),
            'true_negatives': int(tn),
            'false_positives': int(fp),
            'false_negatives': int(fn),
            'roc_auc': roc_auc
        }
        
        logger.info(f"{model_name} Results:")
        logger.info(f"  F1-score: {f1:.4f}")
        logger.info(f"  Precision: {precision:.4f}")
        logger.info(f"  Recall: {recall:.4f}")
        logger.info(f"  Accuracy: {accuracy:.4f}")
        logger.info(f"  False Positive Rate: {fpr:.4f}")
        if roc_auc:
            logger.info(f"  ROC-AUC: {roc_auc:.4f}")
        logger.info(f"  Confusion Matrix:\n{cm}")
        
        # Classification report
        logger.info(f"\nClassification Report:\n{classification_report(y_test, y_pred)}")
        
        return metrics
    
    def save_model(self, model, model_name: str):
        """Save a trained model to disk."""
        model_path = os.path.join(self.models_dir, f"{model_name}.pkl")
        joblib.dump(model, model_path)
        logger.info(f"Saved model to {model_path}")
    
    def load_model(self, model_name: str):
        """Load a trained model from disk."""
        model_path = os.path.join(self.models_dir, f"{model_name}.pkl")
        model = joblib.load(model_path)
        logger.info(f"Loaded model from {model_path}")
        return model
    
    def compare_models(self, X_test: np.ndarray, y_test: np.ndarray) -> pd.DataFrame:
        """
        Compare all trained models on test data.
        
        Args:
            X_test: Test features
            y_test: Test labels
            
        Returns:
            DataFrame with comparison results
        """
        results = []
        
        for model_name, model in self.models.items():
            metrics = self.evaluate_model(model, X_test, y_test, model_name)
            metrics['model'] = model_name
            results.append(metrics)
        
        results_df = pd.DataFrame(results)
        results_df = results_df.set_index('model')
        
        logger.info("\n" + "="*60)
        logger.info("MODEL COMPARISON")
        logger.info("="*60)
        logger.info("\n" + results_df.to_string())
        
        return results_df

