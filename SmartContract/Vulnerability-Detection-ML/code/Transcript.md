# Smart Contract Vulnerability Detection Using Machine Learning
## Presentation Transcript

**Author:** Lucas Kim  
**Course:** CMSI 5350  
**Date:** December 2025

---

## Slide 1: Title Slide

Good [morning/afternoon], everyone. My name is Lucas Kim, and today I'll be presenting my final project for CMSI 5350: "Smart Contract Vulnerability Detection Using Machine Learning: A Pattern-Based Approach to Label Generation and Classification."

---

## Slide 2: Introduction - Motivation

Smart contracts manage billions of dollars in decentralized applications, yet code-level vulnerabilities remain the primary cause of blockchain exploits. Manual security auditing costs $10k to $100k per contract and is extremely time-consuming. This creates a critical need for automated, scalable vulnerability detection systems.

---

## Slide 3: Introduction - Problem Statement

Our research question: Can machine learning models effectively detect vulnerabilities in smart contracts using features extracted from source code?

Our primary objectives: achieve F1-score ≥ 0.85 and false positive rate < 0.10, while processing real-world Ethereum contracts and training multiple classification models.

---

## Slide 4: Introduction - The Critical Challenge

We encountered a fundamental challenge: the labeling problem. The dataset contains 42,908 contracts but provides no vulnerability labels. Without labels, supervised learning is impossible. Synthetic labels - randomly assigning 20% as vulnerable - produced poor performance with F1-scores below 0.30.

**Why This Matters:** This is the core blocker. Without quality labels, even the best ML models fail. Existing tools fail: Slither fails on 99% of contracts due to compilation errors. Manual labeling would take weeks. We needed a breakthrough solution.

---

## Slide 5: Dataset and Preprocessing

We used the Ethereum Smart Contract Dataset from Messi-Q: 42,908 contracts total. Processed 2,000 contracts (2020-2025) for computational efficiency. Preprocessing: directory traversal, encoding error handling, empty file filtering, contract ID generation.

**Data Split:** 80-20 stratified split. Training: 1,600 contracts (71.75% vulnerable). Test: 400 contracts (same distribution). Applied 'balanced' class weighting, random state 42 for reproducibility.

---

## Slide 6: Methodology - Solution Overview

**The Breakthrough:** We developed a hybrid two-tier system that transforms an unsupervised problem into a supervised one. Tier 1: Slither static analysis for contracts that compile - works for only 1% due to compilation failures. Tier 2: Pattern-based detection working directly on source code - achieves 100% coverage. **This is the key innovation:** Every contract receives a meaningful vulnerability assessment based on actual code patterns, enabling supervised learning on unlabeled datasets.

---

## Slide 7: Methodology - Vulnerability Patterns

We defined 9 major vulnerability patterns based on established security best practices.

**High-Severity Patterns (Score +2):**
- **Reentrancy:** External calls followed by state updates - dangerous because calls can re-enter before state is updated
- **Unchecked External Calls:** Calls without return value validation - failed calls can silently fail
- **Delegatecall Usage:** Executes code in calling contract's context - dangerous if user-controlled

**Medium-Severity Patterns (Score +1):**
- **tx.origin usage** - Vulnerable to phishing; should use `msg.sender`
- **Selfdestruct usage** - Can destroy contracts or drain funds
- **Integer overflow (pre-Solidity 0.8)** - Older versions lack overflow protection
- **Uninitialized storage pointers** - Can point to unexpected locations
- **Dangerous low-level calls** - Empty calls can execute fallback functions
- **Missing access control** - Public functions modifying state without modifiers

**Scoring System:** Patterns are additive. Score ≥ 2 → vulnerable. This threshold balances sensitivity and specificity - single minor patterns may be acceptable, but multiple or high-severity patterns indicate genuine risk.

---

## Slide 8: Methodology - Label Generation Process

Our process: collect files, generate contract IDs, attempt Slither analysis first, fallback to pattern-based detection, assign labels, export to JSON.

**Results:** 2,000 contracts - 1,435 vulnerable (71.8%), 565 safe (28.2%). Compilation failures: 1,988 (99.4%). Slither success: only 12 (0.6%). This demonstrates the critical importance of our pattern-based fallback.

**Label Quality:** Labels reflect actual code patterns, not randomness. Patterns align with SWC Registry. Models achieved F1 > 0.95, proving learnable patterns exist.

---

## Slide 9: Methodology - Feature Extraction

We extract 71 features in five categories: **Opcode frequency** (35) - CALL, DELEGATECALL, arithmetic operations, normalized by contract length. **AST node features** (21) - function definitions, modifiers, events. **Control flow** (5) - nesting depth, loop counts. **Code metrics** (7) - lines of code, function count, complexity. **Enhanced features** (39 optional) - Slither-based, disabled for speed.

---

## Slide 10: Methodology - Model Training

We implemented three models: **Logistic Regression** (baseline with L1/L2 regularization), **Random Forest** (ensemble of decision trees), and **XGBoost** (gradient boosting).

Training configuration: 5-fold stratified cross-validation, GridSearchCV with F1-score optimization, 'balanced' class weighting, 80-20 stratified split, random state 42.

---

## Slide 11: Results - Performance Summary

All three models significantly exceeded targets. **Random Forest:** F1 0.9825, Precision 0.9894, Recall 0.9756, FPR 0.0265. **XGBoost:** F1 0.9789, ROC-AUC 0.9926. **Logistic Regression:** F1 0.9607, Precision 0.9853.

**Target Achievement:** F1 target 0.85 → achieved 0.96-0.98 (13-15% above). FPR target < 0.10 → achieved 0.027-0.035 (3x better).

---

## Slide 12: Results - Best Model Analysis

**Random Forest** is our best model: 280 true positives, 110 true negatives, only 3 false positives, 7 false negatives. Highest F1 (0.9825), lowest FPR (0.0265), ideal for production.

---

## Slide 13: Results - Synthetic vs Real Labels

The impact is dramatic. **With synthetic labels:** F1 scores 0.09-0.28. **With real labels:** F1 scores 0.96-0.98 - improvements of 243% to 989%.

**Key Finding:** Label quality is more important than feature quantity or model sophistication. With poor labels, even the best models fail. With good labels, even simple models excel.

---

## Slide 14: Results - Technical Challenges Overcome

We overcame five major challenges: **Missing labels** - solved with pattern-based detection. **99.4% compilation failures** - solved with pattern-based fallback achieving 100% coverage. **Class imbalance (71.8% vulnerable)** - addressed with balanced weighting. **Contract ID matching** - standardized format, 99.4% match rate. **Progress visibility** - added real-time updates.

---

## Slide 15: Conclusion - Key Achievements

We achieved all objectives and exceeded targets: F1-scores 0.96-0.98 (target 0.85), FPR 0.027-0.035 (target < 0.10).

**Technical Contributions:** Pattern-based label generation system, hybrid Slither + pattern matching, 9-pattern vulnerability library, end-to-end production-ready pipeline.

**Validation:** Tested on 2,000 real Ethereum contracts. All three models achieved high performance, demonstrating scalability and robustness.

---

## Slide 16: Conclusion - Impact and Findings

**Key Findings:** ML-based vulnerability detection is feasible with proper label generation. Pattern-based approaches work without compilation. Standard ML models achieve high performance with good features and labels. The approach scales to real-world datasets.

**Primary Innovation - The Breakthrough:** Solving the labeling problem through pattern-based detection **transformed an unsupervised problem into a supervised one**. This is the core contribution: we enabled supervised learning on unlabeled datasets by generating high-quality labels from code patterns, achieving 100% coverage where traditional tools fail 99.4% of the time.

---

## Slide 17: Conclusion - Generalization and Future Work

**Limitations:** Models trained on specific subset. Potential overfitting risk. For production, validation on additional independent datasets is necessary.

**Future Work:** Enhanced label generation, graph-based features, deep learning models, external validation, temporal validation, production deployment with real-time APIs.

---

## Slide 18: Conclusion - Final Thoughts

**The Core Achievement:** The project's success hinged on solving the labeling problem. Our pattern-based vulnerability detection transformed an unsupervised problem into a supervised one, enabling models to achieve target performance - F1 0.96-0.98, FPR 0.027-0.035, exceeding all targets.

**Broader Impact:** This approach extends to other domains where labeled data is scarce but patterns are identifiable. For production deployment: additional validation datasets, continuous monitoring, periodic retraining.

---

## Slide 19: Q&A

Thank you for your attention. I'm now open to questions. I'd be happy to discuss our pattern-based label generation methodology, model generalization concerns, comparisons with GNN-based or rule-based methods, production deployment considerations, or future research directions.

Thank you!

---

## Appendix: Additional Discussion Points

### Technical Challenges

**Five major challenges:** Missing labels (solved with pattern-based detection), compilation failures affecting 99.4% (solved with pattern-based fallback achieving 100% coverage), class imbalance 71.8% vulnerable (addressed with balanced weighting), contract ID matching (standardized format, 99.4% match rate), execution time visibility (added real-time progress updates).

### Comparison with Related Work

**Advantages over GNN-based:** No graph construction overhead, works on non-compilable contracts, faster feature extraction, simpler architecture.

**Advantages over rule-based:** Learns patterns from data, adapts to new vulnerability types, lower false positive rates, scales to large datasets.

### Production Deployment

**Recommendations:** External validation on independent datasets from different sources and time periods, temporal validation on contracts deployed after training, domain adaptation across DeFi, NFT, governance contracts, continuous monitoring and periodic retraining.

---
