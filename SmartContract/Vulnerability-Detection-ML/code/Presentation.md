# Smart Contract Vulnerability Detection Using Machine Learning
## Presentation Slides

**Author:** Lucas Kim  
**Course:** CMSI 5350  
**Date:** December 2025

---

## Slide 1: Title Slide

**Title:** Smart Contract Vulnerability Detection Using Machine Learning

**Subtitle:** A Pattern-Based Approach to Label Generation and Classification

**Author:** Lucas Kim  
**Course:** CMSI 5350  

---

## Slide 2: Introduction - Motivation

**Title:** Why Smart Contract Security Matters

**Key Points:**
- Smart contracts manage billions of dollars in decentralized applications
- Code-level vulnerabilities are the primary cause of blockchain exploits
- Manual security auditing is expensive ($10k–$100k per contract) and time-consuming
- Need for automated, scalable vulnerability detection

**Visual Elements:**
- Statistics on smart contract exploits
- Cost comparison: manual audit vs. automated detection

---

## Slide 3: Introduction - Problem Statement

**Title:** Research Question and Objectives

**Research Question:**
Can machine learning models effectively detect vulnerabilities in smart contracts using features extracted from source code?

**Primary Objectives:**
- Achieve F1-score ≥ 0.85
- Achieve false positive rate < 0.10
- Process real-world Ethereum smart contracts
- Extract comprehensive features from Solidity code
- Train and evaluate multiple classification models

---

## Slide 4: Introduction - The Critical Challenge

**Title:** The Labeling Problem - The Core Blocker

**The Core Issue:**
- Dataset contains 42,908 real contracts but **no vulnerability labels**
- Without labels, supervised learning is impossible
- Synthetic labels (random 20%) produce poor performance (F1 < 0.30)

**Why This Matters:**
This is the fundamental blocker. Without quality labels, even the best ML models fail. Existing tools fail: Slither fails on 99% of contracts due to compilation errors. Manual labeling would take weeks. **We needed a breakthrough solution.**

**Why Labels Were Unavailable:**
1. Raw dataset contains only source code, not annotations
2. Manual labeling of 2,000+ contracts would require weeks of expert work
3. Existing tools (SmartBugs, Slither) have limitations:
   - SmartBugs requires Docker and hours of execution
   - Slither fails on 99% of contracts due to compilation errors

**Visual:** Comparison showing synthetic vs. real label performance

---

## Slide 5: Dataset and Preprocessing

**Title:** Dataset Description and Data Split

**Source:** Ethereum Smart Contract Dataset (Messi-Q/Smart-Contract-Dataset)
- **Total Available:** 42,908 contracts
- **Processed:** 2,000 contracts (subset for computational efficiency)
- **Time Period:** 2020-2025
- **Format:** Solidity source code (.sol files)

**Preprocessing:** Directory traversal, encoding error handling, empty file filtering, contract ID generation

**Data Split:** 80-20 stratified split
- **Training:** 1,600 contracts (71.75% vulnerable)
- **Test:** 400 contracts (71.75% vulnerable)
- **Key Features:** Stratified split, 'balanced' weighting, random state 42

**Visual:** Pie chart showing train/test distribution

---

## Slide 6: Methodology - Solution Overview

**Title:** The Breakthrough - Pattern-Based Vulnerability Detection

**Design Philosophy:**
Instead of relying on external tools or manual annotation, we developed a **hybrid vulnerability detection system** that **transforms an unsupervised problem into a supervised one**:

1. **Tier 1: Slither Analysis** (Primary)
   - Attempts compilation and static analysis
   - Success Rate: ~1% (most contracts fail to compile)

2. **Tier 2: Pattern-Based Detection** (Fallback)
   - Works directly on source code without compilation
   - Success Rate: 100% (works on all contracts)

**Key Innovation - The Breakthrough:**
Ensures every contract receives a meaningful vulnerability assessment based on actual code patterns, not random assignment. **This enables supervised learning on unlabeled datasets** - transforming an impossible problem into a solvable one.

---

## Slide 7: Methodology - Vulnerability Patterns

**Title:** 9 Major Vulnerability Patterns

**High-Severity Patterns (Score: +2):**
1. **Reentrancy:** External calls followed by state updates - dangerous because calls can re-enter before state is updated
2. **Unchecked External Calls:** Calls without return value validation - failed calls can silently fail
3. **Delegatecall Usage:** Executes code in calling contract's context - dangerous if user-controlled

**Medium-Severity Patterns (Score: +1):**
4. **tx.origin usage** - Vulnerable to phishing; should use `msg.sender`
5. **Selfdestruct usage** - Can destroy contracts or drain funds
6. **Integer overflow (pre-Solidity 0.8)** - Older versions lack overflow protection
7. **Uninitialized storage pointers** - Can point to unexpected locations
8. **Dangerous low-level calls** - Empty calls can execute fallback functions
9. **Missing access control** - Public functions modifying state without modifiers

**Scoring System:** Patterns are additive. Score ≥ 2 → vulnerable. Threshold balances sensitivity and specificity.

---

## Slide 8: Methodology - Label Generation Process

**Title:** Label Generation Implementation

**Script:** `generate_labels_with_slither.py`

**Process Flow:**
1. **File Collection:** Scans dataset directory, collecting `.sol` files
2. **Contract ID Generation:** Creates identifiers matching DataLoader format
3. **Dual Analysis:**
   - Attempts Slither analysis first
   - Falls back to pattern-based detection on failure
4. **Label Assignment:** Binary labels (0=safe, 1=vulnerable)
5. **JSON Export:** Saves to `data/labels_slither.json`

**Results:**
- Total Contracts: 2,000
- Vulnerable: 1,435 (71.8%)
- Safe: 565 (28.2%)
- **Vulnerable:Safe Ratio:** 2.54:1
- Compilation Failures: 1,988 (99.4%)
- Slither Success: 12 (0.6%)

**Label Quality Validation:**
- Labels reflect actual code patterns, not randomness
- Detected patterns align with known vulnerability types (SWC Registry)
- Models achieved F1 > 0.95, indicating learnable patterns
- Realistic distribution for uncurated real-world contracts

---

## Slide 9: Methodology - Feature Extraction

**Title:** 71 Comprehensive Features

**Feature Categories:**

1. **Opcode Frequency Features (35 features)**
   - CALL, DELEGATECALL, STATICCALL, SSTORE, SLOAD
   - Arithmetic operations, control flow opcodes
   - Normalized by contract length

2. **AST Node Features (21 features)**
   - FunctionDefinition, ModifierDefinition, EventDefinition
   - VariableDeclaration, control structures
   - Pattern matching approximating AST node counts

3. **Control Flow Features (5 features)**
   - Maximum nesting depth, loop counts
   - External call counts, conditional complexity

4. **Code Metrics (7 features)**
   - Lines of code, character count
   - Function count, state variable count
   - Average function length

5. **Enhanced Features (39 features, optional)**
   - Slither-based: Detector features, CFG metrics, call graph metrics
   - Disabled for speed in current run

---

## Slide 10: Methodology - Model Training

**Title:** Model Training Configuration

**Three Models Implemented:**

1. **Logistic Regression**
   - Baseline with L1/L2 regularization
   - Hyperparameter tuning: C, penalty, solver, max_iter

2. **Random Forest**
   - Ensemble of decision trees
   - Hyperparameter tuning: n_estimators, max_depth, min_samples_split, min_samples_leaf

3. **XGBoost**
   - Gradient boosting classifier
   - Hyperparameter tuning: n_estimators, max_depth, learning_rate, subsample, colsample_bytree, scale_pos_weight

**Training Configuration:**
- Cross-Validation: 5-fold stratified K-fold
- Hyperparameter Tuning: GridSearchCV with F1-score optimization
- Class Weighting: 'balanced' to handle imbalanced data
- Train/Test Split: 80/20 with stratification
- Random State: 42 (for reproducibility)

---

## Slide 11: Results - Performance Summary

**Title:** Model Performance with Real Labels

**Performance Table:**

| Model | F1-Score | Precision | Recall | Accuracy | FPR | ROC-AUC |
|-------|----------|-----------|--------|----------|-----|---------|
| **Logistic Regression** | **0.9607** | 0.9853 | 0.9373 | 0.945 | 0.0354 | 0.9654 |
| **Random Forest** | **0.9825** | 0.9894 | 0.9756 | 0.975 | **0.0265** | 0.9889 |
| **XGBoost** | **0.9789** | 0.9859 | 0.9721 | 0.970 | 0.0354 | **0.9926** |

**Target Achievement:**
- **F1-Score Target:** ≥ 0.85 → **Achieved: 0.96-0.98** (13-15% above target)
- **FPR Target:** < 0.10 → **Achieved: 0.027-0.035** (3x better than target)

**Visual:** Bar chart comparing F1-scores and FPRs

---

## Slide 12: Results - Best Model Analysis

**Title:** Random Forest: Best Overall Model

**Confusion Matrix (Test Set, 400 contracts):**
- **True Positives:** 280 | **True Negatives:** 110
- **False Positives:** 3 | **False Negatives:** 7

**Key Strengths:**
- Highest F1-score (0.9825), lowest FPR (0.0265)
- High precision (0.9894), high recall (0.9756)
- Ideal for production deployment

**Visual:** Confusion matrix heatmap

---

## Slide 13: Results - Synthetic vs Real Labels

**Title:** Impact of Real Labels

**Performance Comparison:**

| Model | Synthetic F1 | Real F1 | Improvement |
|-------|-------------|---------|------------|
| Logistic Regression | 0.28 | **0.96** | **+243%** |
| Random Forest | 0.09 | **0.98** | **+989%** |
| XGBoost | 0.25 | **0.98** | **+292%** |

**Key Finding:** Label quality is more important than feature quantity or model sophistication. With poor labels, even the best models fail. With good labels, even simple models excel.

**Visual:** Side-by-side comparison chart

---

## Slide 14: Results - Technical Challenges Overcome

**Title:** Technical Challenges and Solutions

**Five Major Challenges Overcome:**

1. **Missing Vulnerability Labels** → Pattern-based detection system
2. **99.4% Compilation Failures** → Pattern-based fallback (100% coverage)
3. **Class Imbalance (71.8% vulnerable)** → Balanced weighting, stratified splits
4. **Contract ID Matching** → Standardized format (99.4% match rate)
5. **Progress Visibility** → Real-time updates, time estimation

**Impact:** Enabled supervised learning, achieved target performance, improved user experience

---

## Slide 15: Conclusion - Key Achievements

**Title:** Project Achievements

**Performance Targets Exceeded:**
- F1-score: 0.96-0.98 (Target: ≥0.85)
- False Positive Rate: 0.027-0.035 (Target: <0.10)

**Technical Contributions:**
1. Pattern-based label generation system
2. Hybrid Slither + pattern matching approach
3. Comprehensive vulnerability pattern library (9 patterns)
4. End-to-end production-ready pipeline

**Validation:**
- Tested on 2,000 real Ethereum contracts
- All three models (LR, RF, XGBoost) achieved high performance
- Demonstrated scalability and robustness

---

## Slide 16: Conclusion - Impact and Findings

**Title:** Key Findings

**This work demonstrates that:**
1. **ML-based vulnerability detection is feasible** with proper label generation
2. **Pattern-based approaches** can effectively identify vulnerabilities without compilation
3. **Standard ML models** (LR, RF, XGBoost) can achieve high performance with good features and labels
4. **The approach scales** to real-world datasets

**Primary Innovation - The Breakthrough:**
Solving the labeling problem through pattern-based vulnerability detection **transformed an unsupervised problem into a supervised one**. This is the core contribution: we enabled supervised learning on unlabeled datasets by generating high-quality labels from code patterns, achieving 100% coverage where traditional tools fail 99.4% of the time.

---

## Slide 17: Conclusion - Generalization and Future Work

**Title:** Limitations and Future Work

**Model Generalization:**
- Models trained on specific subset from single dataset
- Potential overfitting risk
- For production: validation on additional independent datasets necessary

**Future Work:**
1. **Enhanced Label Generation:** Multiple static analysis tools, ensemble voting
2. **Feature Engineering:** Graph-based features (CFG, DFG), semantic embeddings
3. **Model Improvements:** Deep learning (LSTM, Transformer), ensemble methods
4. **Evaluation:** External validation, temporal validation, domain adaptation
5. **Production Deployment:** Real-time API, model versioning, explainability (SHAP)

---

## Slide 18: Conclusion - Final Thoughts

**Title:** Summary and Future Directions

**The Core Achievement:**
The project's success hinged on solving the labeling problem. Pattern-based vulnerability detection transformed an unsupervised problem into a supervised one, enabling models to achieve target performance - **F1 0.96-0.98, FPR 0.027-0.035, exceeding all targets by 13-15% and 3x respectively.**

**Broader Applicability:**
This approach extends to other domains where labeled data is scarce but patterns are identifiable.

**Production Considerations:**
- Additional validation datasets from independent sources
- Continuous monitoring and periodic retraining

---

## Slide 19: Q&A

**Title:** Questions and Discussion

**Contact Information:**
- Author: Lucas Kim
- Course: CMSI 5350
- Institution: [Your Institution]

**Key Points for Discussion:**
- Pattern-based label generation methodology
- Model generalization and overfitting concerns
- Comparison with other approaches (GNN-based, rule-based)
- Production deployment considerations
- Future research directions

**Thank You!**

---

## Appendix: Additional Slides (If Needed)

### Slide A1: Technical Challenges and Solutions

**Challenge 1: Missing Vulnerability Labels**
- Solution: Pattern-based vulnerability detection system
- Impact: Enabled supervised learning

**Challenge 2: Compilation Failures (99.4%)**
- Solution: Pattern-based fallback system
- Impact: 100% coverage regardless of compilation status

**Challenge 3: Class Imbalance (71.8% vulnerable)**
- Solution: 'balanced' class weighting, stratified splits
- Impact: Balanced precision and recall

**Challenge 4: Contract ID Matching**
- Solution: Standardized contract ID format
- Impact: 99.4% label matching rate

**Challenge 5: Execution Time and Progress Visibility**
- Solution: Real-time progress updates, estimated remaining time
- Impact: Improved user experience

---

### Slide A2: Code Structure

**Project Structure:**
```
code/
├── main.py                          # Main pipeline execution
├── generate_labels_with_slither.py  # Pattern-based label generation
├── tune_hyperparameters.py          # Hyperparameter optimization
├── generate_statistics.py           # Visualization generation
├── src/
│   ├── data_loader.py              # Data loading with label support
│   ├── feature_extractor.py        # 71-feature extraction
│   ├── slither_extractor.py        # Slither-based features (optional)
│   ├── smartbugs_loader.py         # SmartBugs label loader
│   └── models.py                   # Model training and evaluation
├── data/
│   └── labels_slither.json         # Generated vulnerability labels
├── models/                          # Trained model files
└── results/                         # Evaluation results and visualizations
```

---

### Slide A3: Vulnerability Pattern Examples

**Reentrancy Example:**
```solidity
// Vulnerable pattern detected:
contract.call{value: amount}("");
balances[msg.sender] = 0;  // State update after call
```

**Unchecked External Call Example:**
```solidity
// Vulnerable: no return value check
someAddress.call{value: amount}("");

// Safe: return value checked
require(someAddress.call{value: amount}(""), "Call failed");
```

---

